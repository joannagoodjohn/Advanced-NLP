{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2019 - Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, December 4th (before class)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**NOTE**<br><br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-3.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle. In case of questions, you can contact the TAs or David via the usual channels.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In this assignment, you will implement a bigram part-of-speech (POS) tagger based on Hidden Markov Models from scratch. Using NLTK is disallowed, except for the modules explicitly listed below. For this, you will need to develop and utilize the following modules:<br>\n",
    "1. Corpus reader and writer<br>\n",
    "2. Training procedure, including smoothing<br>\n",
    "3. Viterbi tagging, including unknown word handling <br>\n",
    "4. Evaluation<br>\n",
    "The task is mostly very straightforward, but each step requires careful design. Thus, we suggest you proceed in the following way.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import HTML\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "from nltk.probability import (\n",
    "    FreqDist,\n",
    "    ConditionalFreqDist,\n",
    "    ConditionalProbDist,\n",
    "    DictionaryProbDist,\n",
    "    DictionaryConditionalProbDist,\n",
    "    LidstoneProbDist,\n",
    "    MutableProbDist,\n",
    "    MLEProbDist,\n",
    "    RandomProbDist,\n",
    ")\n",
    "#Importing libraries\n",
    "import nltk, re, pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint, time\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Viterbi Algorithm [33 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "First, implement the Viterbi algorithm for finding the optimal state (tag) sequence given the sequence of observations (words). <br><br>\n",
    "In order to test your implementation, verify that you compute the correct state sequence for some examples from Eisner's ice cream model (see lecture) for which the solutions are known.<br><br>\n",
    "Demonstrate that your algorithm computes the correct state sequence for ['3','1','3'] as in the lecture.<br><br>\n",
    "Make sure that your algorithm is correct before proceeding to the other tasks! In order to do this, please also test your module with a longer observation sequence. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Distributions\n",
      "{'C': 0.2, 'H': 0.8}\n",
      "\n",
      "Transition Probabilities\n",
      "Sequence:['C', 'C', 'C']                                              Score:0.001440\n",
      "Sequence:['C', 'C', 'H']                                              Score:0.002880\n",
      "Sequence:['C', 'H', 'C']                                              Score:0.000480\n",
      "Sequence:['C', 'H', 'H']                                              Score:0.003360\n",
      "Sequence:['H', 'C', 'C']                                              Score:0.001536\n",
      "Sequence:['H', 'C', 'H']                                              Score:0.003072\n",
      "Sequence:['H', 'H', 'C']                                              Score:0.001792\n",
      "Sequence:['H', 'H', 'H']                                              Score:0.012544\n",
      "\n",
      " Best Sequence\n",
      "['H', 'H', 'H'] 0.012544000000000001\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(states,sequence_length):\n",
    "    \n",
    "    all_sequences = []\n",
    "    nodes = []\n",
    "    \n",
    "    depth = sequence_length\n",
    "    \n",
    "    def gen_seq_recur(states,nodes,depth):\n",
    "        if depth == 0:\n",
    "            \n",
    "            all_sequences.append(nodes)\n",
    "        else:\n",
    "            for state in states:\n",
    "                temp_nodes = list(nodes)\n",
    "                temp_nodes.append(state)\n",
    "                gen_seq_recur(states,temp_nodes,depth-1)\n",
    "    \n",
    "    gen_seq_recur(states,[],depth)\n",
    "                \n",
    "    return all_sequences\n",
    "\n",
    "def score_sequences(sequences,initial_probs,transition_probs,emission_probs,obs):\n",
    "    \n",
    "    best_score = -1\n",
    "    best_sequence = None\n",
    "    \n",
    "    sequence_scores = []\n",
    "    for seq in sequences:\n",
    "        total_score = 1\n",
    "        total_score_breakdown = []\n",
    "        first = True\n",
    "        for i in range(len(seq)):\n",
    "            state_score = 1\n",
    "            # compute transitition probability score\n",
    "            if first == True:\n",
    "                state_score *= initial_probs[seq[i]]\n",
    "                # reset first flag\n",
    "                first = False\n",
    "            else:  \n",
    "                state_score *= transition_probs[seq[i] + \"|\" + seq[i-1]]\n",
    "            # add to emission probability score\n",
    "            state_score *= emission_probs[obs[i] + \"|\" + seq[i]]\n",
    "            \n",
    "            total_score_breakdown.append(state_score)\n",
    "            total_score *= state_score\n",
    "            \n",
    "        sequence_scores.append(total_score)\n",
    "        \n",
    "    return sequence_scores\n",
    "\n",
    "\n",
    "def initializeSequences(_obs):\n",
    "    # Generate list of sequences\n",
    "    \n",
    "    seqLen = len(_obs)\n",
    "    seqs = generate_sequence(states,seqLen)\n",
    "    # Score sequences\n",
    "    seq_scores = score_sequences(seqs,initial_probs,transition_probs,emission_probs,obs)\n",
    "    \n",
    "    return (seqLen,seqs,seq_scores)\n",
    "\n",
    "states = ['C','H']\n",
    "obs = ['1','2','3']\n",
    "initial_probs = {'C': 0.2, 'H': 0.8}\n",
    "transition_probs = {'C|C':0.6, 'C|H': 0.4, 'H|C':0.3, 'H|H':0.7}\n",
    "emission_probs = {'1|C':0.5,'2|C':0.4,'3|C':0.1, '1|H':0.2, '2|H':0.4,'3|H':0.4}\n",
    "\n",
    "\n",
    "\n",
    "sequence_length,sequences,sequence_scores = initializeSequences(obs)\n",
    "# print results\n",
    "print(\"Initial Distributions\")\n",
    "print (initial_probs)\n",
    "print(\"\\nTransition Probabilities\")\n",
    "\n",
    "# Generate list of sequences\n",
    "sequence_length,sequences,sequence_scores = initializeSequences(obs)\n",
    "# Display sequence scores\n",
    "for i in range(len(sequences)):\n",
    "    print(\"Sequence:%-60s Score:%0.6f\" % (sequences[i],sequence_scores[i]))\n",
    "    \n",
    "# Display the winning score\n",
    "print(\"\\n Best Sequence\")\n",
    "print(sequences[sequence_scores.index(max(sequence_scores))],max(sequence_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: HMM Training [33 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Second, learn parameters, i.e. the initial, transition, and emission probabilities, for your HMM from *annotated* data. Implement a maximum likelihood training procedure (with smoothing) for supervised learning of HMMs. We will be using the Wall Street Journal corpus (part of the Penn Treebank). It is a licensed corpus, so please do not redistribute the files. The zip archive provided on Moodle contains a training set, a test set, and an evaluation set. The training set (`wsj_tagged_train.tt`) and the evaluation set (`wsj_tagged_eval.tt`) are written in the commonly used CoNLL format. They are text files with two colums; the first column contains the words, the POS tags are in the second column, and empty lines delimit sentences. The test set (`wsj_tagged_test.t`) is a copy of the evaluation set with tags stripped. You should tag this test set using your tagger and then compare your results with the gold-standard ones in the provided tagged evaluation file. The corpus uses the Penn Treebank tagset.<br><br>\n",
    "You are welcome to use any NLTK data structures from the two modules `nltk.corpus.reader` (and submodules) and `nltk.probability`. The latter includes a number of smoothing procedures, which you may want to apply to your corpus frequency counts. Take care to get NLTK to make the smoothed probability distributions sum to one. Experiment with unsmoothed distributions, Laplace add-one smoothing, and at least one further smoothing procedure.<br><br>\n",
    "Note that your tagger will initially fail to produce output for sentences that contain words you haven't seen in training. If you have such a word $w$ appear at sentence position $t$, you will have $b_j(w) = 0$ for all states/tags $j$, and therefore $V_t(j) = 0$ for all $j$. Adapt your tagger by implementing the following crude approach to unknown words. Whenever you get $V_t(j) = 0$ for all $j$ because of an unknown word at position $t$, pretend that $b_j(w) = 1$ for all $j$. This will basically set $V_t(j) = max_iV_{t-1}(i)a_{ij}$, and allow you to interpolate the missing POS tag based on the transition probabilities alone.<br><br>\n",
    "Note 2: In order to avoid additional problems with zero-probability transitions when applying your model to the test set, make sure that you tag the corpus sentence by sentence (i.e., compute the optimal tag sequence for each sentence independently). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploring Tagged Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_train = \"wsj_tagged_train.tt\"\n",
    "CORPUS_gold = \"wsj_tagged_eval.tt\"\n",
    "CORPUS_test = \"wsj_tagged_test.t\"\n",
    "\n",
    "\n",
    "\n",
    "def read_annotated(fname):\n",
    "    '''Read the sentences from an annotated corpus.'''\n",
    "    with open(fname, 'r') as f:\n",
    "        sentences = [[]]\n",
    "        for l in f.readlines():\n",
    "            s = l.split()\n",
    "            if s:\n",
    "                # Next word, append to sentence.\n",
    "                sentences[-1].append((s[0].lower(), s[1]))\n",
    "            else:\n",
    "                # Sentence has ended.\n",
    "                sentences.append([])\n",
    "\n",
    "        # Get rid of '[]' in the end.\n",
    "        sentences.pop()\n",
    "        return sentences\n",
    "\n",
    "\n",
    "def read_unannotated(fname):\n",
    "    '''Read sentences from an unannotated corpus.'''\n",
    "    with open(fname, 'r') as f:\n",
    "        sentences = [[]]\n",
    "        for l in f.readlines():\n",
    "            s = l.strip().lower()\n",
    "            if s:\n",
    "                # Next word, append to sentence.\n",
    "                sentences[-1].append(s)\n",
    "            else:\n",
    "                # Sentence has ended.\n",
    "                sentences.append([])\n",
    "\n",
    "        # Get rid of '[]' in the end.\n",
    "        sentences.pop()\n",
    "        return sentences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test\n",
    "train_set = read_annotated(CORPUS_train)\n",
    "test_set = read_unannotated(CORPUS_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of tagged words\n",
    "train_tagged_words = [tup for sent in train_set for tup in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'an', 'oct.', '19', 'review', 'of', '``', 'the', 'misanthrope', \"''\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens \n",
    "tokens = [pair[0] for pair in train_tagged_words]\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NN', 'VBD', 'LS', 'RBS', '.', 'NNP', 'RBR', '#', ':', 'RB', 'TO', 'NNS', '``', 'VBP', 'PRP', 'FW', 'VBG', 'WDT', 'CC', 'JJR', 'RP', 'WP', 'IN', 'VBN', 'PDT', 'EX', 'MD', \"''\", 'VB', 'POS', ')', 'VBZ', 'JJ', '$', 'JJS', 'NNPS', 'WP$', ',', '(', 'CD', 'UH', 'PRP$', 'WRB', 'DT', 'SYM'}\n"
     ]
    }
   ],
   "source": [
    "# vocabulary\n",
    "V = set(tokens)\n",
    "# number of tags\n",
    "T = set([pair[1] for pair in train_tagged_words])\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. POS Tagging Algorithm - HMM\n",
    "\n",
    "HMM algorithm is used in order to,assign the most probable tag to the word, given a sequence of words to be tagged, the task is to \n",
    "In general assign the tag that maximises the likelihood P(t/w). \n",
    "\\\n",
    "\\\n",
    "$P(t/w) = P(w/t). P(t) / P(w)$\n",
    "\\\n",
    "\\\n",
    "where, \n",
    "\\\n",
    "\\\n",
    "P(w/t) is  the probability that given a tag, what is the probability of it being w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing P(w/t) and storing in T x V matrix\n",
    "t = len(T)\n",
    "v = len(V)\n",
    "w_given_t = np.zeros((t, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute word given tag: Emission Probability\n",
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    tag_list = [pair for pair in train_bag if pair[1]==tag]\n",
    "    count_tag = len(tag_list)\n",
    "    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n",
    "    count_w_given_tag = len(w_given_tag_list)\n",
    "    \n",
    "    return (count_w_given_tag, count_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n",
    "\n",
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    tags = [pair[1] for pair in train_bag]\n",
    "    count_t1 = len([t for t in tags if t==t1])\n",
    "    count_t2_t1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if tags[index]==t1 and tags[index+1] == t2:\n",
    "            count_t2_t1 += 1\n",
    "    return (count_t2_t1, count_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-fdca0728417e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtags_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2_given_t1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt2_given_t1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-b8f317575436>\u001b[0m in \u001b[0;36mt2_given_t1\u001b[0;34m(t2, t1, train_bag)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mt2_given_t1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tagged_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcount_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcount_t2_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b8f317575436>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mt2_given_t1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_tagged_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcount_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcount_t2_t1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creating t x t transition matrix of tags\n",
    "# each column is t2, each row is t1\n",
    "# thus M(i, j) represents P(tj given ti)\n",
    "\n",
    "tags_matrix = np.zeros((len(T), len(T)), dtype='float32')\n",
    "for i, t1 in enumerate(list(T)):\n",
    "    for j, t2 in enumerate(list(T)): \n",
    "        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "tags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(words, train_bag = train_tagged_words):\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "    \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = tags_df.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = tags_df.loc[state[-1], tag]\n",
    "                \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = word_given_tag(words[key], tag)[0]/word_given_tag(words[key], tag)[1]\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "            \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#alternative method for emissions probabilities\n",
    "def calc_initial(sentences):\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for s in sentences:\n",
    "        # Count only the first tag in each sentence.\n",
    "        tag = s[0][1]\n",
    "        d[tag] += 1\n",
    "\n",
    "    # Normalize.\n",
    "    total = sum(d.values())\n",
    "    return {k: v/total for k, v in d.items()}\n",
    "\n",
    "\n",
    "def calc_transitions(sentences):\n",
    "   \n",
    "    bigrams = []\n",
    "    for s in sentences:\n",
    "        [bigrams.append((tag1, tag2)) for (_, tag1), (_, tag2) in zip(s, s[1:])]\n",
    "\n",
    "    d = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for (b1, b2) in bigrams:\n",
    "        d[b1][b2] += 1\n",
    "\n",
    "    # Normalize.\n",
    "    ret = {}\n",
    "    for tag, dist in d.items():\n",
    "        total = sum(dist.values())\n",
    "        ret[tag] = {k: v/total for k, v in dist.items()}\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def calc_emissions(sentences):\n",
    "    d = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for word, tag in chain.from_iterable(sentences):\n",
    "        d[tag][word] += 1  # Note reversed ordering compared to the corpus!\n",
    "\n",
    "    # Normalize.\n",
    "    ret = {}\n",
    "    for tag, dist in d.items():\n",
    "        total = sum(dist.values())\n",
    "        ret[tag] = {k: v/total for k, v in dist.items()}\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class Train:\n",
    "   \n",
    "    def __init__(self, fname=CORPUS_train):\n",
    "        self.sen = read_annotated(fname)\n",
    "\n",
    "\n",
    "    def get_initial(self):\n",
    "        return calc_initial(self.sen)\n",
    "\n",
    "\n",
    "    def get_transitions(self):\n",
    "        return calc_transitions(self.sen)\n",
    "\n",
    "\n",
    "    def get_emissions(self):\n",
    "        return calc_emissions(self.sen)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sen = read_annotated(CORPUS_train)\n",
    "    print(calc_initial(sen))\n",
    "    #print(calc_transitions(sen))\n",
    "    #print(calc_vocabulary_size(sen))\n",
    "    #print(calc_emissions(sen))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Explain which smoothing procedures you used and any observations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your text goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Evaluation [34 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Once you have trained a model, evaluate it on the unseen data from the test set. Run the Viterbi algorithm with each of your models, and output a tagged corpus in the two-column CoNLL format (*.tt). Use the provided evaluation script `tagging_eval.py`, which you can run on a gold annotated file and your own tagging results.<br><br>\n",
    "Run it on the output of your tagger and the evaluation set and report your results. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sents\n",
    "test_run = [test_set[i] for i in len(test_set)]\n",
    "\n",
    "# list of tagged words\n",
    "test_run_base = [tup for sent in test_run for tup in sent]\n",
    "\n",
    "# list of untagged words\n",
    "test_tagged_words = [tup[0] for sent in test_run for tup in sent]\n",
    "test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'rstrip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ae42ebeba169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mw_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfsock_gold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m              \u001b[0mgsplitline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m              \u001b[0mtline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsock_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m              \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'rstrip'"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    import string\n",
    "    \n",
    "    try:\n",
    "        fsock_gold=read_unannotated(CORPUS_gold)\n",
    "        fsock_test=read_unannotated(CORPUS_test)\n",
    "        sentences = 0\n",
    "        s_right = 0\n",
    "        words = 0\n",
    "        w_right = 0\n",
    "        for gline in fsock_gold:\n",
    "             gsplitline=gline.rstrip().split()\n",
    "             tline = fsock_test.readline() \n",
    "             if not tline: break\n",
    "             tsplitline=tline.rstrip().split()\n",
    "             glinelen = len(gsplitline)\n",
    "             tlinelen = len(tsplitline)\n",
    "             if not glinelen == tlinelen:\n",
    "                 print ('Unmatched gold standard and test')\n",
    "                 print ('gold: %s' % gline)\n",
    "                 print ('test: %s' % tline)\n",
    "                 break\n",
    "             index=glinelen-1\n",
    "             serror=0\n",
    "             if glinelen>0:\n",
    "                 while index > -1:\n",
    "                     words = words + 1\n",
    "                     gelem=gsplitline[index]\n",
    "                     telem=tsplitline[index] \n",
    "                     if gelem == telem:\n",
    "                         w_right = w_right +1\n",
    "                     else: serror=1\n",
    "                     index = index - 1\n",
    "                 sentences = sentences + 1\n",
    "                 if not serror: s_right = s_right + 1\n",
    "             else: continue\n",
    "        fsock_gold.close()\n",
    "        fsock_test.close()\n",
    "        print ('Computing scores')\n",
    "        print ('%d right out of %d tags correct\\n' % (w_right, words))\n",
    "        print ('%.2f per cent word accuracy\\n' % (float(w_right)/float(words) * 100.0, ))\n",
    "        print ('%.2f per cent sentence accuracy\\n' % (float(s_right)/float(sentences) * 100.0, ))\n",
    "    except IOError:\n",
    "        print (\"Unable to open %s or %s\" % (gold_file, test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Discuss the results of the different tagger versions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your discussion goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The task is challenging as it stands. However, feel free to go further for extra credit, e.g. by doing one of the following: implement better unknown word handling, use a trigram tagger, plot a learning curve for your tagger (accuracy as a function of training data size), plot a speed vs. sentence length curve.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning curve of tagger\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(train_sizes, train_F1_mean, label = 'Training error')\n",
    "plt.plot(train_sizes, validation_F1_mean, label = 'Validation error')\n",
    "plt.ylabel('Score', fontsize = 14)\n",
    "plt.xlabel('Training set size', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.ylim(0,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your discussion*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "ctrl-q"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
